
"""
Creates a multiprocess, multithread application to detect high
readings.

See https://www.assemblesoftware.com/examples/

"""

import sys
import os
import math

sys.path.append(os.path.abspath("../../IoTPy/multiprocessing"))
sys.path.append(os.path.abspath("../../IoTPy/core"))
sys.path.append(os.path.abspath("../../IoTPy/agent_types"))
sys.path.append(os.path.abspath("../../IoTPy/helper_functions"))

# multicore is in multiprocessing
from multicore import shared_memory_process, Multiprocess
# stream is in core
from stream import Stream
# op, merge, source, sink are in agent_types
from op import map_element, map_window
from merge import zip_map, merge_window
from source import source_float_file
from sink import stream_to_file
# print_stream is in helper functions. Useful for debugging.
from print_stream import print_stream

# ----------------------------------------------------------------
# COMPUTE FUNCTION f FOR THE PROCESS THAT GENERATES LOCAL ANOMALIES
# ----------------------------------------------------------------
def f(in_streams, out_streams):
    """
    Parameters
    ----------
    in_streams: list of Stream
      in_streams is usually a list of 3 streams indicating
      measurements in x, y and z directions or equivalently
      in e, n, and z (for east, north, vertical) directions.
      Each triaxial sensor generates x, y and z streams.
    out_streams: list of Stream
      out_streams has only one element, which is a
      Stream of int. An element of this stream is either
      1.0 or 0.0. An element is 1.0 to indicate that an
      anomaly was detected and is 0.0 otherwise.

    """
    
    #----------------------------------------------------
    # DECLARE INTERNAL STREAMS
    #----------------------------------------------------
    # zero_means is an array of streams, with one stream
    # for each stream in in_streams. Usually zero_means[0, 1, 2]
    # represent streams in the x, y, z direction generated by a single
    # triaxial sensor.
    zero_means = [
        Stream('zero_means_' + str(i)) for i in
        range(len(in_streams))]
    # magnitudes is a stream of magnitudes of a vector from its x, y,
    # z values.
    magnitudes = Stream('magnitudes')

    #----------------------------------------------------
    # CREATE AGENTS
    #----------------------------------------------------
    for i in range(len(in_streams)):
        subtract_mean(in_stream=in_streams[i],
                      out_stream=zero_means[i],
                      window_size=500)
    magnitude_of_vector(in_streams=zero_means,
                        out_stream=magnitudes)
    anomalies(in_stream=magnitudes,
              out_stream=out_streams[0])


def local_anomaly_process(filenames):
    shared_memory_process(
        compute_func=f,
        in_stream_names=directions, out_stream_names=['out'],
        connect_sources=[
            (directions[i], source_file_floats(filenames[i]))
            for i in range(len(directions))])


# --------------------------------------------------------
# THE COMPUTE FUNCTION g FOR THE AGGREGATION PROCESS
# --------------------------------------------------------    
def g(in_streams, out_streams):
    """
    Parameters
    ----------
    in_streams: list of Stream
      in_streams is a list of anomaly streams with one stream from
      each sensor. An anomaly stream is a sequence of 0.0 and 1.0
      where 0.0 indicates no anomaly and 1.0 indicates an anomaly.
    out_streams: list of Stream
      This list consists of a single stream that contains 0.0
      when no global anomaly across all sensors is detected and 1.0
      when a global anomaly is detected.

    """
    #----------------------------------------------------
    # DECLARE INTERNAL STREAMS
    #----------------------------------------------------
    regional_anomalies = Stream('Regional anomalies')

    #----------------------------------------------------
    # CREATE AGENTS
    #----------------------------------------------------

    # Create the aggregation agent
    #----------------------------------------------------    
    # Define the terminating function
    def aggregate(windows):
        number_local_anomalies = [
            any(window) for window in windows].count(True)
        return 1.0 if number_local_anomalies > 1 else 0.0
    # Wrap the terminating function to create an agent
    merge_window(
        func=aggregate,
        in_streams=in_streams, out_stream=out_streams[0],
        window_size=250, step_size=1, initial_value=0.0)

    # Agents that copy streams to a file
    # Plot these files to understand the application and debug.
    for i in range(len(in_streams)):
        stream_to_file(in_streams[i], 'local_anomalies_'+str(i+1)+'_.txt')
    stream_to_file(out_streams[0], 'regional_anomalies.txt')


# DEFINE PROCESSES
#local_anomaly_processes = []
NUM_STEPS=40000
TIME_INTERVAL=0.0005

# Sensor data in horizontal (e, n) and vertical (z)
# directions for 3 sensors.
source_files = [
    ['S0515.HNE.txt', 'S0515.HNN.txt', 'S0515.HNZ.txt'],
    ['S0516.HNE.txt', 'S0516.HNN.txt', 'S0516.HNZ.txt'],
    ['S0517.HNE.txt', 'S0517.HNN.txt', 'S0517.HNZ.txt']
    ]

sensor_data_file_dict = {
    'S1': ['S1.e.txt', 'S1.n.txt', 'S1.z.txt'],
    'S2': ['S2.e.txt', 'S2.n.txt', 'S2.z.txt'],
    'S3': ['S3.e.txt', 'S3.n.txt', 'S3.z.txt']
    }

def source_file_floats(filename):
    return [
        source_float_file(
            filename, TIME_INTERVAL, NUM_STEPS).source_func
        for filename in filenames]

def local_anomaly_process(filenames):
    shared_memory_process(
        compute_func=f,
        in_stream_names=directions, out_stream_names=['out'],
        connect_sources=[
            (directions[i], source_file_floats(filenames[i]))
            for i in range(len(directions))])

        
sensors = {'S1':
           {'e': 'S0515.HNE.txt',
            'n': 'S0515.HNN.txt',
            'z': 'S0515.HNZ.txt'
           },
           'S2':
           {'e': 'S0516.HNE.txt',
            'n': 'S0516.HNN.txt',
            'z': 'S0516.HNZ.txt'
           },
           'S3':
           {'e': 'S0517.HNE.txt',
            'n': 'S0517.HNN.txt',
            'z': 'S0517.HNZ.txt'
           }
         }


directions = ['e', 'n', 'z']
# source_sensor_direction is a dict, key is sensor_name,
# value is a dict where key is a direction and value is
# a source.
source_sensor_direction = {}
for sensor_name in sensors.keys():
    source_sensor_direction[sensor_name] = {}
    for direction in directions:
        source_sensor_direction[sensor_name][direction] = \
          source_float_file(
              filename=sensors[sensor_name][direction],
              time_interval=TIME_INTERVAL,
              num_steps=NUM_STEPS).source_func

source_processes = [
    shared_memory_process(
        compute_func=f,
        in_stream_names=['e', 'n', 'z'],
        out_stream_names=['out'],
        connect_sources=[
            (direction,
             source_sensor_direction[sensor_name][direction])
            for direction in ['e', 'n', 'z']])
    for sensor_name in sensors.keys()]

aggregation_process = shared_memory_process(
    compute_func=g,
    in_stream_names=[
      'in_'+ str(i) for i in range(len(sensors))],
    out_stream_names=['out'],
    connect_sources=[])

vm = Multiprocess(
    processes=source_processes + [aggregation_process],
    connections = [
        (source_processes[i],  'out',
         aggregation_process, 'in_'+str(i))
        for i in range(len(sensors))])
vm.run()
